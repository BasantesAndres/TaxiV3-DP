ğŸš• Taxi-v3 with Dynamic Programming (DP)

Policy Evaluation Â· Policy Iteration Â· Value Iteration on Gymnasiumâ€™s Taxi-v3.
This project computes optimal policies in a tabular MDP using exact model-based DP and then evaluates/visualizes performance with animations, plots, and summary tables.

âœ¨ What you get

Exact DP solvers for Taxi-v3:

Policy Evaluation (iterative)

Policy Iteration (evaluation + greedy improvement)

Value Iteration (Bellman optimality backups + greedy extraction)

Transition model from the env (env.unwrapped.P) to do exact backups

Testing harness with episode rollouts, metrics (avg return, std, success rate, avg steps) and live animation (matplotlib)

Visualizations: comparison bar charts + value-function heatmap slice

Results table printed to console and pickled to taxi_dp_results.pkl for your report

ğŸ—‚ Project structure
.
â”œâ”€â”€ TaxiFinal.py              # Main script with DP algorithms and plots
â”œâ”€â”€ README.md                 # (this file)
â””â”€â”€ taxi_dp_results.pkl       # (created after running; results dict for report)

ğŸ§° Requirements

Python â‰¥ 3.9

Packages

pip install "gymnasium[toy_text]" numpy matplotlib pandas pygame


Notes
â€¢ gymnasium[toy_text] ensures Taxi-v3 is available.
â€¢ pygame helps with rendering on some systems.
â€¢ If youâ€™re on Conda:

conda install -c conda-forge gymnasium pygame numpy matplotlib pandas

â–¶ï¸ How to run
python TaxiFinal.py


What youâ€™ll see:

Console logs for Policy Iteration and Value Iteration convergence (Î” residuals / iteration counts).

Test episodes (rewards, steps, success) for each learned policy.

Plots: bar charts comparing algorithms, and a value-function heatmap.

A pickle file taxi_dp_results.pkl with all results for your report.

Tip: The first 3 test episodes are animated in a small matplotlib window (when render=True inside the script).

ğŸ§ª What does the script do?

Creates the environment and reads the exact transition model P
(env.unwrapped.P â†’ probabilities, next state, reward, done) â€” needed for exact DP.

Runs Policy Iteration: alternating Policy Evaluation + greedy Policy Improvement until policy is stable.

Runs Value Iteration: Bellman optimality sweeps on 
ğ‘‰
V until Î” < Î¸, then greedy policy extraction.

Evaluates each learned policy over episodes; prints Avg Reward, Std, Success Rate, Avg Steps and iterations to converge.

Plots a 2Ã—2 comparison dashboard (rewards, success rate, convergence iterations, steps) and a value heatmap slice (Passenger=Red, Destination=Green).

âš™ï¸ Configuration & flags

The environment supports optional stochastic toggles:

is_raining=True â†’ movement noise: 0.8 to intended direction, 0.1 left, 0.1 right.

fickle_passenger=True â†’ once after first pickup and moving one step away from the source, passenger switches destination with 30% probability.

To try them for evaluation (or training if you re-extract the model), edit the env creation in the script, e.g.:

# After solver = TaxiDynamicProgramming(...)
solver.env = gym.make('Taxi-v3', is_raining=True, fickle_passenger=True)
solver._build_transition_model()  # refresh P from this env (for exact DP)


DP remains correct iff the P you use in backups reflects those probabilities.
MC/TD can also be compared under these flags (they learn from samples).

ğŸ“Š Interpreting the outputs

Convergence logs

Policy Iteration: â€œPolicy Evaluation converged in â€¦ iterationsâ€ multiple times, plus a final â€œPolicy Iteration converged in N iterations!â€ (outer loops).

Value Iteration: â€œâ€¦ converged in M iterations!â€ once residual Î” < Î¸.

Results table
Shows Convergence (iterations), Avg Reward, Std Reward, Success Rate (%), Avg Steps per algorithm. Higher Avg Reward / lower Avg Steps is better on Taxi-v3.

Plots

Rewards & Steps bars: performance comparison.

Success Rate bar: reliability.

Convergence iterations: computational effort.

Heatmap: value 
ğ‘‰
(
ğ‘ 
)
V(s) across taxi positions for a fixed passenger/destination slice (Passenger=Red, Destination=Green). Brighter = better start states.

ğŸ§© Reproducibility tips

Fix Python and Gymnasium versions in your environment (e.g., requirements.txt).

If you need fixed episode behavior, add seeds in the code:

state, _ = test_env.reset(seed=42)
np.random.seed(42)


On headless Linux, you may need a non-interactive matplotlib backend:

import matplotlib; matplotlib.use("Agg")

ğŸ›  Troubleshooting

Plots donâ€™t show â†’ Ensure you run as a script (python TaxiFinal.py) or enable interactive backends.

Pygame warning on Windows â†’ Itâ€™s benign; you can ignore it or upgrade setuptools/pygame.

Very slow first evaluation â†’ Normal for random initial policy; later evaluations converge faster.
